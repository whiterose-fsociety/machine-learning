{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:lightgreen\">Information Gain</h2>\n",
    "\n",
    "<h3> Collection of methods that handle different scenarios for dirrent representations of data </h3>\n",
    "\n",
    "<h4 style=\"color:lightblue\"> #TODO - Sanity Checks </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:lightgreen\">Entropy methods</h3>\n",
    "<h4 style=\"color:lightblue\">Free to edit </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:pink\">Inputs</h3>\n",
    "<h4>Option of entering just dataframe or both dataframe and feature name(s)</h4>\n",
    "<h5>Pandas Dataframe</h5>\n",
    "<h5>Feature/s</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:lightblue\">One line implementations of entropy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_entropy(dataset,column):\n",
    "\tentropy = abs(sum([(x/sum(list(dataset.iloc[:,column].value_counts())))*np.log2(x/sum(list(dataset.iloc[:,column].value_counts()))) for x in list(dataset.iloc[:,column].value_counts())]))\n",
    "\treturn entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Second implementation\n",
    "def lazy_entropy(dataset,column):\n",
    "\tentropy = abs(sum([(x/sum(list(dataset.iloc[:,column].value_counts())))*np.log2(x/sum(list(dataset.iloc[:,column].value_counts()))) for x in list(dataset.iloc[:,column].value_counts())]))\n",
    "\treturn entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Column values of a pandas Dataframe\n",
    "def entropy_feature(dataset,column):\n",
    "  unique_values = list(dataset[column].value_counts())\n",
    "  probs = [x/sum(unique_values) for x in unique_values]\n",
    "  # [-0.6428571428571429, -0.35714285714285715]\n",
    "  entropy = sum([prob*np.log2(prob) for prob in probs])\n",
    "  return abs(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Pass in dataset, assumes that last column is the target class </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(dataset):\n",
    "  unique_values =  list(dataset.iloc[:,-1].value_counts())\n",
    "  probs = [x/sum(unique_values) for x in unique_values]\n",
    "  entropy = sum([prob*np.log2(prob) for prob in probs])\n",
    "  return abs(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:pink\">Information Gain Methods </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain_feature(dataset,feature):\n",
    "\t# Original dataset\n",
    "\tunique_values = list(dataset[feature].value_counts())\n",
    "\t# Names of the features\n",
    "\tunique_index = dataset[feature].value_counts().index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tinfo_gain = 0\n",
    "\tdata_entropy = entropy(dataset)\n",
    "\tdata_size = dataset.shape[0]\n",
    "\n",
    "\tentropies = []\n",
    "\t# Finds Entropies and returns list of entropy and size\n",
    "\tfor unique_feature in unique_index:\n",
    "\t\tfilt = dataset[feature]\t== unique_feature\n",
    "\t\t# Create new dataset according to feature split\n",
    "\t\tfeature_dataset = dataset[filt]\t\n",
    "\t\t# [([entropies,size],[])]\n",
    "\t\tentropies.append((entropy(feature_dataset),feature_dataset.shape[0]))\n",
    "\n",
    "\n",
    "\tbranch_entropy = sum([ data[0] * data[1] for data in entropies])\n",
    "\tinfo_gain = data_entropy - branch_entropy/data_size\n",
    "\t\n",
    "\treturn info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gains_features(dataset,features):\n",
    "\tinfo_gains = {}\n",
    "\tfeature_size = len(features)\n",
    "\tfor feature,size in zip(features,range(feature_size)):\n",
    "\t\tinfo_gains[feature] = info_gain_feature(dataset,feature)\n",
    "\treturn info_gains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:lightgreen\"> Second Implementation </h4>\n",
    "<p><strong>Returns a dictionary of information gains for each feature. Where key = column index in the table and value = information gain </strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample return  = {0: 0.24674981977443899, 1: 0.029222565658954647, 2: 0.15183550136234147, 3: 0.048127030408269267}\n",
    "def info_gains(dataset):\n",
    "\tcols = dataset.shape[1]\n",
    "\tinfo_gains = {}\n",
    "\tfor col in range(cols-1):\n",
    "\t\tunique_values = list(dataset.iloc[:,col].value_counts())\n",
    "\t\tunique_index = dataset.iloc[:,col].value_counts().index.tolist()\n",
    "\n",
    "\t\tinfo_gain = 0\n",
    "\t\tdata_entropy = entropy(dataset)\n",
    "\t\tdata_size = dataset.shape[0]\n",
    "\n",
    "\t\tentropies = []\n",
    "\n",
    "\t\tfor unique_feature in unique_index:\n",
    "\t\t\tfilt = dataset.iloc[:,col] == unique_feature\n",
    "\t\t\tfeature_dataset = dataset[filt]\n",
    "\t\t\tentropies.append((entropy(feature_dataset),feature_dataset.shape[0]))\n",
    "\n",
    "\t\tbranch_entropy = sum([ data[0] * data[1] for data in entropies])\n",
    "\t\tinfo_gain = data_entropy - branch_entropy/data_size\t\t\n",
    "\n",
    "\t\t# info_gains.append((info_gain,col))\n",
    "\t\tinfo_gains[col] = info_gain\n",
    "\treturn info_gains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:lightgray\">Find Highest Information Gain </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_attribute(gains):\n",
    "\tx  = []\n",
    "\tfor gain in gains.values():\n",
    "\t\tx.append(gain)\n",
    "\tm = max(x)\n",
    "\tindex = list(gains.keys())[list(gains.values()).index(m)]\n",
    "\treturn index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:lightgreen\"> Run Method to see the values of Information Gain</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:lightblue\">Example with Feature Names given </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Column: Information  {'Outlook': 0.24674981977443899, 'Temperature': 0.029222565658954647, 'Humidity': 0.15183550136234147, 'Windy': 0.048127030408269267, 'Play': 0.94028595867063092}\n",
      "\n",
      "Feature Column Index Play\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"tennis.csv\",header=None,names=[\"Outlook\",\"Temperature\",\"Humidity\",\"Windy\",\"Play\"])\n",
    "features=[\"Outlook\",\"Temperature\",\"Humidity\",\"Windy\",\"Play\"]\n",
    "gains = info_gains_features(dataset,features)\n",
    "print(\"Feature Column: Information \",gains) # Dictionary\n",
    "print()\n",
    "print(\"Feature Column Index\" ,best_attribute(gains))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:lightgreen\">Example with only dataset given </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Column: Information  {0: 0.24674981977443899, 1: 0.029222565658954647, 2: 0.15183550136234147, 3: 0.048127030408269267}\n",
      "\n",
      "Feature Column Index 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = pd.read_csv(\"tennis.csv\",header=None,names=[\"Outlook\",\"Temperature\",\"Humidity\",\"Windy\",\"Play\"])\n",
    "gains = info_gains(dataset)\n",
    "print(\"Feature Column: Information \",gains) # Dictionary\n",
    "print()\n",
    "print(\"Feature Column Index\" ,best_attribute(gains))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:pink\"> ID3 ALGORITHM</h2>\n",
    "<p><strong>ID3 is an algorithm that is mostly used to generate a decision tree , invented by <i>Ross Quinlan</i></strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3_features(sub_dataset,dataset,features,parent_node_target=None):\n",
    "\t#If target values have the same value\n",
    "\tif len(np.unique(sub_dataset.iloc[:,-1])) <=1 :\n",
    "\t\treturn np.unique(sub_dataset.iloc[:,-1])[0]\n",
    "\t#If sub_dataset is empty\t\n",
    "\telif len(sub_dataset) == 0:\n",
    "\t\treturn parent_node_target\n",
    "\n",
    "\t#If feature list is empty\n",
    "\telif len(features) == 0:\n",
    "\t\ttarget_dict = sub_dataset.iloc[:,-1].value_counts().to_dict()\n",
    "\t\treturn best_attribute(target_dict)\n",
    "\telse:\n",
    "\t\tparent_node_dict = sub_dataset.iloc[:,-1].value_counts().to_dict()\n",
    "\t\tparent_node_target = best_attribute(parent_node_dict)\n",
    "\t\tgains = info_gains_features(sub_dataset,features)\n",
    "\t\tbest_feature = best_attribute(gains)\n",
    "\n",
    "\t\ttree = {best_feature:{}}\n",
    "\n",
    "\n",
    "\t\tfeatures.remove(best_feature)\n",
    "\n",
    "\t\t# Grow a branch under the root\n",
    "\t\tfeature_values = sub_dataset[best_feature].value_counts().index.tolist()\n",
    "\t\tfor feature_value in feature_values:\n",
    "\t\t\tfilt = sub_dataset[best_feature] == feature_value\n",
    "\t\t\tsub = sub_dataset[filt]\n",
    "\t\t\tsubtree = ID3_features(sub,dataset,features,parent_node_target)\n",
    "\n",
    "\t\t\ttree[best_feature][feature_value] = subtree\n",
    "\n",
    "\t\treturn (tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <strong>Incase you don't have a list of features </strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3(sub_dataset,dataset):\n",
    "\tfeatures = list(dataset.columns)\n",
    "\tfeatures.remove(features[-1])\n",
    "\treturn ID3_features(sub_dataset,dataset,features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:lightgreen\">Print the tree</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2df5a2325d51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tennis.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Outlook\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Temperature\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Humidity\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Windy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Play\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#features=[\"Outlook\",\"Temperature\",\"Humidity\",\"Windy\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = pd.read_csv(\"tennis.csv\",header=None,names=[\"Outlook\",\"Temperature\",\"Humidity\",\"Windy\",\"Play\"])\n",
    "#features=[\"Outlook\",\"Temperature\",\"Humidity\",\"Windy\"]\n",
    "print(ID3(dataset,dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
