{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Type of data we have\n",
    "1. Class: no-recurrence-events, recurrence-events\n",
    "2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
    "3. menopause: lt40, ge40, premeno.\n",
    "4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n",
    "5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n",
    "6. node-caps: yes, no.\n",
    "7. deg-malig: 1, 2, 3.\n",
    "8. breast: left, right.\n",
    "9. breast-quad: left-up, left-low, right-up, right-low, central.\n",
    "10. irradiat: yes, no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv('dataset.csv')\n",
    "dataset = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Removing datapoints with missing values\n",
    " * we remove occurances of the question mark symbol (?)\n",
    " * The Function return a data set with no datapoints containing missing values and a dataset only containing data with missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteMissing(data):\n",
    "    indices = np.array([])\n",
    "    missingValues = np.empty((0,10))\n",
    "    dataset = data\n",
    "    for i in  range(len(dataset)):\n",
    "        for j in range(len(dataset[i])):\n",
    "            if dataset[i][j] == '?':\n",
    "                indices = np.append(indices , [int(i)])\n",
    "                missingValues = np.append(missingValues , np.array([dataset[int(i)]]), axis = 0 )\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        # We start deleting in from the dataset from the end of the indices array\n",
    "        dataset = np.delete(dataset , int(indices[len(indices)-1-i]) , 0 )\n",
    "    return (dataset , missingValues);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting appropriate datasets\n",
    "* we start with a dataset the will have no occurences of data points with missing values\n",
    "* Next is a dataset of all datapoints with missing values\n",
    "* Finally we set set make a data set similar to the dataset withouth missing values which we will later change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset , dataWithMissingValues = deleteMissing(dataset)\n",
    "datasetNMV = dataset # We later make this dataset a complete dataset with no occurences of missing data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Below we split the data into training , testing and validating data \n",
    "* We first shuffle the data to randomise it\n",
    "* We then split the data accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193, 10) (84, 10)\n"
     ]
    }
   ],
   "source": [
    "# Give the data a random shuffle first\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Data split\n",
    "trainingData , testData = dataset[:int(.70 * dataset.shape[0])] , dataset[int(.70 * dataset.shape[0]):]\n",
    "\n",
    "print(trainingData.shape , testData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Priors\n",
    "* The function below receives an array of classes\n",
    "* It then returns an array of the amount of classes are there\n",
    "* It also return an array of how many times each class appears in the input Array\n",
    "* Lastly It returns an array of probabilities for each class\n",
    "* This is to be used inside the Naive Bayes function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Priors(classArray):\n",
    "    probabilities = np.array([])\n",
    "    classes , counts = np.unique(classArray , return_counts = True)\n",
    "    \n",
    "    for i in counts:\n",
    "        probabilities = np.append(probabilities , [i/np.sum(counts)])\n",
    "    \n",
    "    return (classes , counts , probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes\n",
    "* This class receives and index, idexing a feature to be used as an array of classes we are trying to classify\n",
    "* It also receives a matric of training data\n",
    "* it also receives a datapoint we are trying to classify\n",
    "* `The Function returns an array of probabilities and an array of classes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayes(classIndex , trainData , dataPoint):\n",
    "    classes , classCounts , priorProbs = Priors(trainData[:,classIndex])\n",
    "    totalProbs = np.array([])\n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        \n",
    "        prob = priorProbs[i]\n",
    "        Class = classes[i]\n",
    "        classOccurance = classCounts[i]\n",
    "        \n",
    "        for j in range(len(dataPoint)):\n",
    "            \n",
    "            if j == classIndex:\n",
    "                continue\n",
    "                \n",
    "            fCount = 0\n",
    "            \n",
    "            for k in range(len(trainData)):\n",
    "                if len(np.where(trainData[k][classIndex] == Class and trainData[k][j] == dataPoint[j])[0] > 0):\n",
    "                    fCount += 1 \n",
    "            \n",
    "            fCount = fCount/classOccurance\n",
    "            prob *= fCount \n",
    "            \n",
    "        totalProbs = np.append(totalProbs , [prob])\n",
    "    \n",
    "    return (totalProbs , classes);\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dealing with missing Values\n",
    "* We now need to predict what the missing values are using our Naive bayes classifier\n",
    "* Once we have done that we need to add those data points back to the main data set\n",
    "* Then lastly we need to run our classifier on the complete data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember we have a matric of missing data points named dataWithMissingvalues\n",
    "\n",
    "def fixMissingValues(data , missingData):\n",
    "    dataset = data\n",
    "    miss = missingData\n",
    "    newData = np.empty((0,10))\n",
    "    for dataPoint in miss:\n",
    "        d = dataPoint      ## This was for testing, it works just fine so no need to change it\n",
    "        if dataPoint[5] == '?':\n",
    "            probs , classes = NaiveBayes(5 , dataset , dataPoint)\n",
    "            d[5] = classes[np.argmax(probs)]\n",
    "            dataset = np.append(dataset , [d] , axis = 0)\n",
    "        elif dataPoint[8] == '?':\n",
    "            probs , classes = NaiveBayes(8 , dataset , dataPoint)\n",
    "            d[8] = classes[np.argmax(probs)]\n",
    "            dataset = np.append(dataset , [d] , axis = 0)\n",
    "      \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetNMV is the Dataset with no occurences of missing values\n",
    "* From this data we will create a training and a testing set\n",
    "* Which we will later use for reporting accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetNMV = fixMissingValues(datasetNMV, dataWithMissingValues)\n",
    "np.random.shuffle(datasetNMV)\n",
    "TrainSet , TestSet = datasetNMV[:int(.70*len(datasetNMV))] , datasetNMV[int(.70*len(datasetNMV)):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Confusion Matrix\n",
    "* Below is a generalized confusion matrix taking in the training set and testing set\n",
    "* The Function returns a confusion matric which we will use to analyse and deduce information for our model\n",
    "* We are going to run the Model on the complete data set as well as the set with no missing value occurances\n",
    "> `Information :` \n",
    " * index is the feature index\n",
    " * TP is For TRUE POSITIVE\n",
    " * TN is TRUE NEGATIVE\n",
    " *  FN is FALSE NEGATIVE\n",
    " * FP is fALSE POSITIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatric(trainingSet , testingSet):\n",
    "    index = 0\n",
    "    TP = 0 \n",
    "    TN = 0 \n",
    "    FN = 0 \n",
    "    FP = 0\n",
    "    \n",
    "    for dataPoint in testingSet:\n",
    "        probs , classes = NaiveBayes(index , trainingSet , dataPoint)\n",
    "        predicted = classes[np.argmax(probs)]\n",
    "        trueValue = dataPoint[index]\n",
    "        \n",
    "        if trueValue == 'recurrence-events':\n",
    "            if predicted == trueValue:\n",
    "                TP += 1\n",
    "            else : \n",
    "                FN += 1\n",
    "        else:\n",
    "            if predicted == trueValue:\n",
    "                TN += 1\n",
    "            else : \n",
    "                FP += 1\n",
    "    \n",
    "    \n",
    "    ConfusionMatrix = np.array([[TP , FP],[FN , TN]])\n",
    "    return ConfusionMatrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "> Here we write a fucntion that receives a confusion matric this functions returns: \n",
    "* The Accuracy of the model\n",
    "* The Specificity (AKA True Negative Rate)\n",
    "* The Precision , Ration of correct Positive examples to Total positive examples\n",
    "* The Recall, ratio of the number of correct positive examples out of those that were classified as positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analysis(confusionMatric):\n",
    "    Tp = confusionMatric[0][0]\n",
    "    Fp = confusionMatric[0][1]\n",
    "    Fn = confusionMatric[1][0]\n",
    "    Tn = confusionMatric[1][1]\n",
    "    \n",
    "    accuracy = (Tp + Tn) / (Tp + Fp + Fn + Tn)\n",
    "    specificity = Tn / (Tn + Fp)\n",
    "    precision = Tp / (Tp + Fp)\n",
    "    recall = Tp / (Tp + Fn)\n",
    "    \n",
    "    return(accuracy , specificity , precision , recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averages\n",
    "* Below we run the algorithm again and again for a given dataset\n",
    "* This function basically prints out stuff and returns nothing\n",
    "* This function prints out the evarage amount Analysis of the date\n",
    "* The function receives a dataset and the amount of times one would want the algorithm to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Averages(data , amount):\n",
    "    dataset = data\n",
    "    TP = 0 \n",
    "    TN = 0 \n",
    "    FN = 0 \n",
    "    FP = 0\n",
    "    \n",
    "    for i in range(amount):\n",
    "        np.random.shuffle(dataset)\n",
    "        # Data split\n",
    "        trainingData , testData = dataset[:int(.70 * dataset.shape[0])] , dataset[int(.70 * dataset.shape[0]):]\n",
    "        \n",
    "        confusionMatric = ConfusionMatric(trainingData, testData)\n",
    "        TP += confusionMatric[0][0]\n",
    "        FP += confusionMatric[0][1]\n",
    "        FN += confusionMatric[1][0]\n",
    "        TN += confusionMatric[1][1]\n",
    "        \n",
    "    ConfusionMatrix = np.array([[TP , FP],[FN , TN]])\n",
    "    accuracy , specificity , precision , recall = Analysis(ConfusionMatrix)\n",
    "    print(\"The average accuracy of the given dataset is: \", accuracy)\n",
    "    print(\"The average recall of the given dataset is: \", recall)\n",
    "    print(\"The average specificity of the given dataset is: \" , specificity)\n",
    "    print(\"The average precision of the given dataset is: \" , precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally Reporting Averages\n",
    "* We have 2 datasets, one complete one and the other is one where the occurences of missing values have been removed\n",
    "* Below we report averages of both these datasets\n",
    "* We are going to start with the data with removed occurences then with the full data set\n",
    "* This is run randomising test and training set 100, you can change this value in the input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy of the given dataset is:  0.7288095238095238\n",
      "The average recall of the given dataset is:  0.4765974765974766\n",
      "The average specificity of the given dataset is:  0.8330809355544337\n",
      "The average precision of the given dataset is:  0.5413777161349976\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Averages(dataset,100)\n",
    "print()\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print()\n",
    "Averages(datasetNMV,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
